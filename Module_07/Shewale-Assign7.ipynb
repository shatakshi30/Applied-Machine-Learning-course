{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61764043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27820 entries, 0 to 27819\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   country             27820 non-null  object \n",
      " 1   year                27820 non-null  int64  \n",
      " 2   sex                 27820 non-null  object \n",
      " 3   age                 27820 non-null  object \n",
      " 4   suicides_no         27820 non-null  int64  \n",
      " 5   population          27820 non-null  int64  \n",
      " 6   suicides/100k pop   27820 non-null  float64\n",
      " 7   country-year        27820 non-null  object \n",
      " 8   HDI for year        8364 non-null   float64\n",
      " 9    gdp_for_year ($)   27820 non-null  object \n",
      " 10  gdp_per_capita ($)  27820 non-null  int64  \n",
      " 11  generation          27820 non-null  object \n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 2.5+ MB\n",
      "#total= 27820\n",
      "#duplicated= 0\n",
      "Missing values: country                   0\n",
      "year                      0\n",
      "sex                       0\n",
      "age                       0\n",
      "suicides_no               0\n",
      "population                0\n",
      "suicides/100k pop         0\n",
      "country-year              0\n",
      "HDI for year          19456\n",
      " gdp_for_year ($)         0\n",
      "gdp_per_capita ($)        0\n",
      "generation                0\n",
      "is_duplicate              0\n",
      "dtype: int64\n",
      "N rows=8364 M columns=13\n",
      "country                object\n",
      "year                    int64\n",
      "sex                    object\n",
      "age                    object\n",
      "suicides_no             int64\n",
      "population              int64\n",
      "suicides/100k pop     float64\n",
      "country-year           object\n",
      "HDI for year          float64\n",
      " gdp_for_year ($)      object\n",
      "gdp_per_capita ($)      int64\n",
      "generation             object\n",
      "is_duplicate             bool\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8364 entries, 72 to 27819\n",
      "Columns: 108 entries, year to generation_Silent\n",
      "dtypes: float64(4), uint8(104)\n",
      "memory usage: 1.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'datasets/master.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data_info = data.info()\n",
    "data_head = data.head()\n",
    "\n",
    "# Check for duplicates, this adds a new column to the dataset\n",
    "data[\"is_duplicate\"] = data.duplicated()\n",
    "\n",
    "# Note that when using f-strings, the internal quote character must be different, such as 'is_duplicate' above\n",
    "print(f\"#total= {len(data)}\")\n",
    "print(f\"#duplicated= {len(data[data['is_duplicate']==True])}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "print(f'N rows={len(data)} M columns={len(data.columns)}')\n",
    "print(data.dtypes)\n",
    "\n",
    "# Dropping the 'HDI for year' Column\n",
    "data.drop(columns=['HDI for year', 'suicides_no', ' gdp_for_year ($) ', 'country-year', 'is_duplicate'], inplace=True)\n",
    "\n",
    "# One-Hot Encoding for Categorical Variables\n",
    "categorical_cols = ['country', 'sex', 'age', 'generation']\n",
    "data = pd.get_dummies(data, columns=categorical_cols)\n",
    "    \n",
    "# Normalization/Standardization\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['year', 'population', 'suicides/100k pop', 'gdp_per_capita ($)']\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec436bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regression coefficients: 107\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = data.drop(columns=['suicides/100k pop']) \n",
    "y = data['suicides/100k pop']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the number of regression coefficients\n",
    "num_coefficients = len(model.coef_)\n",
    "print(\"Number of regression coefficients:\", num_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73df115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.3723764400203052\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Filter the original DataFrame based on the specified conditions\n",
    "subset_data = data[(data['age_15-24 years'] == 1) & \n",
    "                   (data['sex_male'] == 1) &\n",
    "                   (data['generation_Generation X'] == 1)]\n",
    "\n",
    "# Define the input data for prediction\n",
    "input_data = subset_data.drop(columns=['suicides/100k pop'])  # Remove the target variable\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "prediction = model.predict(input_data)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "true_values = subset_data['suicides/100k pop']  # Actual values from the subset\n",
    "mae = mean_absolute_error(true_values, prediction)\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce78e2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country                  0\n",
      "year                     0\n",
      "sex                      0\n",
      "age                      0\n",
      "population               0\n",
      "suicides/100k pop        0\n",
      "gdp_per_capita ($)       0\n",
      "generation            5844\n",
      "dtype: int64\n",
      "Missing values in 'generation' after imputation: 0\n",
      "   country      year  sex  age  population  suicides/100k pop  \\\n",
      "0        0 -1.683615    1    2   -0.391617               6.71   \n",
      "1        0 -1.683615    1    4   -0.392870               5.19   \n",
      "2        0 -1.683615    0    2   -0.397548               4.83   \n",
      "3        0 -1.683615    1    6   -0.466035               4.59   \n",
      "4        0 -1.683615    1    3   -0.401485               3.28   \n",
      "\n",
      "   gdp_per_capita ($)  generation  \n",
      "0           -0.850864         4.0  \n",
      "1           -0.850864         2.0  \n",
      "2           -0.850864         4.0  \n",
      "3           -0.850864         1.0  \n",
      "4           -0.850864         3.0  \n"
     ]
    }
   ],
   "source": [
    "# Question 3 \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'datasets/master.csv'\n",
    "data1 = pd.read_csv(file_path)\n",
    "\n",
    "# Check for duplicates, this adds a new column to the dataset\n",
    "data1[\"is_duplicate\"]= data1.duplicated()\n",
    "\n",
    "# Dropping redundant columns\n",
    "data1.drop(columns=['HDI for year', 'suicides_no', ' gdp_for_year ($) ', 'country-year', 'is_duplicate'], inplace=True)\n",
    "\n",
    "# Handling Categorical Variables\n",
    "# For 'country' and 'sex', continue using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data1['country'] = label_encoder.fit_transform(data1['country'])\n",
    "data1['sex'] = label_encoder.fit_transform(data1['sex'])\n",
    "\n",
    "# For 'generation', use a mapping to preserve natural order\n",
    "generation_order = {'G.I. Generation': 1, 'Silent': 2, 'Boomers': 3, 'Generation X': 4, 'Millennials': 5, 'Generation Z': 6}\n",
    "data1['generation'] = data1['generation'].map(generation_order)\n",
    "\n",
    "# For 'age', sort numerically (you might need to adjust this depending on how age is formatted in your dataset)\n",
    "age_order = {'5-14 years': 1, '15-24 years': 2, '25-34 years': 3, '35-54 years': 4, '55-74 years': 5, '75+ years': 6}\n",
    "data1['age'] = data1['age'].map(age_order)\n",
    "\n",
    "# Checking for missing values in each column\n",
    "missing_values = data1.isnull().sum()\n",
    "\n",
    "# Display the missing values\n",
    "print(missing_values)\n",
    "\n",
    "# Find the most frequent value (mode) in the 'generation' column\n",
    "mode_generation = data1['generation'].mode()[0]\n",
    "\n",
    "# Fill missing values in 'generation' with the mode\n",
    "data1['generation'].fillna(mode_generation, inplace=True)\n",
    "\n",
    "# Verify if there are any missing values left in 'generation'\n",
    "missing_values = data1['generation'].isnull().sum()\n",
    "print(\"Missing values in 'generation' after imputation:\", missing_values)\n",
    "\n",
    "# Normalization/Standardization of Numerical Variables\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['year', 'population', 'gdp_per_capita ($)']\n",
    "data1[numerical_cols] = scaler.fit_transform(data1[numerical_cols])\n",
    "\n",
    "# Check the processed data\n",
    "print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346977a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of line coefficients: 6\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "\n",
    "# Build a new model using the original numerical form of 'sex', 'age', and 'generation'\n",
    "X = data1[['year', 'sex', 'age', 'generation', 'population', 'gdp_per_capita ($)']]\n",
    "y = data1['suicides/100k pop']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Number of line coefficients\n",
    "num_coefficients = len(model.coef_)\n",
    "print(\"Number of line coefficients:\", num_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a1d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted target value: [14.0331118]\n",
      "MAE: 7.323111796377767\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "\n",
    "# Prepare the input data for prediction\n",
    "input_data = pd.DataFrame({\n",
    "    'year': [0],  \n",
    "    'sex': [1],   # Male sex\n",
    "    'age': [2],   # Age 20\n",
    "    'generation': [4],  # Generation X\n",
    "    'population': 0,     \n",
    "    'gdp_per_capita ($)': 0  \n",
    "})\n",
    "\n",
    "# Use the model to make predictions\n",
    "prediction = model.predict(input_data)\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Predicted target value:\", prediction)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "true_value = data1.loc[(data1['age'] == 2) & (data1['sex'] == 1) & (data1['generation'] == 4), 'suicides/100k pop'].values[0]\n",
    "mae = mean_absolute_error([true_value], prediction)\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effbed0",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "**Model from Question 1**\n",
    "- **Approach**: Used a preprocessed dataset where 'sex', 'age', and 'generation' were one-hot encoded.\n",
    "- **Regression Coefficients**: The model had 107 coefficients, indicating a high dimensionality due to one-hot encoding.\n",
    "- **Prediction and MAE for Specific Case (Age 20, Male, Generation X)**: Calculated a MAE of 0.3723764400203052. This value is specific to the subset of data representing 20-year-old males from Generation X.\n",
    "\n",
    "**Model from Question 3**\n",
    "- **Approach**: Used the original form of 'sex', 'age', and 'generation' variables, which were then feature-engineered into numerical features.\n",
    "- **Regression Coefficients**: The model had 6 coefficients, indicating lower dimensionality compared to the one-hot encoded model.\n",
    "- **Prediction and MAE for Specific Case (Age 20, Male, Generation X)**: The prediction was made for the same demographic group as in question 1, and the MAE was 7.323111796377767.\n",
    "\n",
    "**Comparative Analysis**\n",
    "1. **Complexity**: The first model is more complex due to the higher number of coefficients resulting from one-hot encoding. The second model is simpler with fewer coefficients, indicating lower complexity.\n",
    "\n",
    "2. **Interpretability**: The second model might be easier to interpret due to the fewer number of coefficients and direct numerical transformation of 'age' and 'generation'.\n",
    "\n",
    "3. **Performance**: Based on the MAE values, the first model (one-hot encoded) shows a lower error rate for the specific case of predicting the suicide rate for 20-year-old males from Generation X. However, it's important to note that MAE values are highly context-specific.\n",
    "\n",
    "4. **Generalization**: Lower complexity models (like the second model) often generalize better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bad0330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted target value: [21.82531268]\n"
     ]
    }
   ],
   "source": [
    "# Question 6\n",
    "\n",
    "# Prepare the input data for prediction\n",
    "input_data = pd.DataFrame({\n",
    "    'year': [0],  \n",
    "    'sex': [1],   # Male sex\n",
    "    'age': [3],   # Age 33\n",
    "    'generation': [7],  # Generation Alpha\n",
    "    'population': [0],     \n",
    "    'gdp_per_capita ($)': [0]  \n",
    "})\n",
    "\n",
    "# Use the model to make predictions\n",
    "prediction = model.predict(input_data)\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Predicted target value:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f9569",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "One significant advantage of using regression (as opposed to classification with nominal features) in the context of independent variables is its ability to capture and utilize continuous and ordinal relationships among these variables. This characteristic is particularly beneficial when dealing with real-world datasets where variables often exist on a continuum or have a natural order, and their precise values carry meaningful and quantifiable information. Here's a more detailed breakdown of this advantage:\n",
    "\n",
    "**Capturing Continuous Relationships:**\n",
    "- **Precision and Nuance**: Regression models can interpret and use the exact values of continuous independent variables, allowing for a more nuanced and precise understanding of their impact on the dependent variable. This is particularly valuable in cases where slight changes in an independent variable could lead to significant changes in the outcome.\n",
    "  \n",
    "- **Interpretability of Coefficients**: In regression, the coefficients of continuous independent variables directly represent the change in the dependent variable for a unit change in the predictor. This interpretability is crucial in many fields like economics, medicine, and social sciences, where understanding the strength and direction of relationships is essential.\n",
    "\n",
    "**Utilizing Ordinal Relationships:**\n",
    "- **Ordinal Data Handling**: When dealing with ordinal data (data with a natural order, like 'low', 'medium', 'high'), regression can capture the intrinsic ordering in these variables, which is often lost in classification tasks. This can lead to more accurate and meaningful models, especially in cases where the order itself has an impact on the outcome.\n",
    "\n",
    "**Summary:**\n",
    "In summary, regression offers a robust framework for incorporating the exact or ordered values of independent variables into the model, leading to potentially more accurate predictions and richer insights, especially in scenarios where the subtleties and nuances in variable values play a crucial role in determining the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d654366",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "One significant advantage of using regular numerical values rather than one-hot encoding for regression is related to model simplicity and dimensionality. When regular numerical values (especially for ordinal categorical variables) are used instead of one-hot encoding, it often leads to simpler models with fewer input features, which can offer several benefits:\n",
    "\n",
    "**Reduced Model Complexity and Lower Dimensionality:**\n",
    "- **Avoiding the Curse of Dimensionality**: One-hot encoding transforms categorical variables into multiple binary variables, one for each category. This can significantly increase the number of features in the dataset, leading to high-dimensional models. Using numerical values keeps the feature space smaller, helping to avoid the \"curse of dimensionality\" which can hamper model performance, particularly in terms of overfitting and computational efficiency.\n",
    "  \n",
    "- **Easier Interpretation and Analysis**: Models with fewer features are generally easier to interpret and analyze. Each coefficient in a regression model directly corresponds to one feature, so fewer features mean a more straightforward interpretation of how each variable affects the outcome.\n",
    "  \n",
    "- **Improved Computational Efficiency**: With fewer features, models often require less computational resources for training and prediction, making them more efficient to run. This can be especially beneficial when dealing with large datasets or when computational resources are limited.\n",
    "\n",
    "- **Handling Ordinal Data Appropriately**: If the categorical variable is ordinal (where the order of categories is meaningful), encoding it as a regular numerical value preserves this order, which can lead to a more accurate model. One-hot encoding, on the other hand, treats each category as separate and equal, losing any ordinal information.\n",
    "\n",
    "**Summary:**\n",
    "In essence, using regular numerical values can simplify the model, making it more interpretable, computationally efficient, and potentially more accurate (especially in the case of ordinal data) compared to the complex, high-dimensional models that can result from one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a133b",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "\n",
    "I would suggest using a regression model to the machine learning model customer. Hereâ€™s a more detailed justification:\n",
    "\n",
    "1. **Nature of Target Variable**: The target variable in the problem is a continuous quantity, representing a rate. Regression is well-suited for predicting continuous outcomes and can provide precise, quantifiable predictions, which is essential for tasks like estimating suicide rates.\n",
    "\n",
    "2. **Interpretability**: Regression models offer valuable interpretability, especially in terms of understanding how changes in independent variables like age, sex, and generation influence the suicide rate. Each coefficient in a regression model represents the expected change in the suicide rate for a unit change in a predictor, assuming other variables are held constant. This can provide insightful and actionable information.\n",
    "\n",
    "3. **Subtlety and Nuance in Predictions**: Regression can capture the subtleties and nuances in the data, offering a detailed prediction. This is important in cases like yours where small changes in predictors could significantly influence the outcome.\n",
    "\n",
    "4. **Model Performance and Suitability**: Based on the tasks in the assignment, the regression model has been tailored to predict a specific rate, aligning closely with the continuous nature of your target variable. This direct alignment often results in better performance for the given task.\n",
    "\n",
    "In conclusion, while classification could categorize data into discrete classes (like high or low suicide risk), regression provides a more fitting approach for your task of predicting an exact numerical value. Its ability to provide detailed, continuous outputs and the interpretability of its results makes it the recommended choice for your machine learning customer in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8295f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
