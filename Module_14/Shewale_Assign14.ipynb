{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQVDorbB4GCd",
        "outputId": "24e4696d-22ed-4dca-af75-0b265856aac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K8-6gnoB4P__"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file (Adjust as per your Google Drive structure)\n",
        "zip_path = '/content/drive/MyDrive/AML Mod 13 data/archive 5.49.00â€¯PM.zip'\n",
        "\n",
        "# Destination folder where the zip file will be extracted\n",
        "extract_to = '/content/sample_data/'\n",
        "\n",
        "# Unzipping the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wql4o3BW4Q6G"
      },
      "outputs": [],
      "source": [
        "# Question1\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "IMGSIZE = (128, 128)\n",
        "CNAMES = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
        "\n",
        "# Define paths for training and testing data\n",
        "base_path = extract_to\n",
        "train_path = os.path.join(base_path, 'seg_train/seg_train/')\n",
        "test_path = os.path.join(base_path, 'seg_test/seg_test/')\n",
        "\n",
        "X_tr, y_tr = [], []\n",
        "X_ts, y_ts = [], []\n",
        "\n",
        "# Load training data\n",
        "for label in CNAMES:\n",
        "    path = train_path + label\n",
        "    for f in sorted([_ for _ in os.listdir(path) if _.lower().endswith('.jpg')]):\n",
        "        img = cv2.imread(os.path.join(path, f))\n",
        "        img_resized = cv2.resize(img, IMGSIZE)\n",
        "        X_tr.append(img_resized)\n",
        "        y_tr.append(CNAMES.index(label))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_tr = np.array(X_tr)\n",
        "y_tr = np.array(y_tr)\n",
        "\n",
        "# Load test data\n",
        "X_ts, y_ts = [], []\n",
        "for label in CNAMES:\n",
        "    path = test_path + label\n",
        "    for f in sorted([_ for _ in os.listdir(path) if _.lower().endswith('.jpg')]):\n",
        "        img = cv2.imread(os.path.join(path, f))\n",
        "        img_resized = cv2.resize(img, IMGSIZE)\n",
        "        X_ts.append(img_resized)\n",
        "        y_ts.append(CNAMES.index(label))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_ts = np.array(X_ts)\n",
        "y_ts = np.array(y_ts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A9kn9Xr75gWO"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Convert data to PyTorch tensors and create datasets\n",
        "tensor_x_tr = torch.Tensor(X_tr) / 255.0  # Scale images to [0, 1]\n",
        "tensor_x_tr = tensor_x_tr.permute(0, 3, 1, 2)  # Reshape to [N, C, H, W]\n",
        "tensor_y_tr = torch.Tensor(y_tr).long()\n",
        "\n",
        "tensor_x_ts = torch.Tensor(X_ts) / 255.0\n",
        "tensor_x_ts = tensor_x_ts.permute(0, 3, 1, 2)\n",
        "tensor_y_ts = torch.Tensor(y_ts).long()\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_ts, tensor_y_ts) # training dataset actually has the test data\n",
        "test_dataset = TensorDataset(tensor_x_tr, tensor_y_tr)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G2P4XJ196PBS"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjHIWxSj6PCR",
        "outputId": "3208569b-cbd8-45b2-c473-ec0eff0cf5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed, Loss: 1.6862241657490427\n",
            "Epoch 2 completed, Loss: 1.0510969301487536\n",
            "Epoch 3 completed, Loss: 0.9075314751330842\n",
            "Epoch 4 completed, Loss: 0.7939764432450558\n",
            "Epoch 5 completed, Loss: 0.6213615752281026\n",
            "Epoch 6 completed, Loss: 0.3801120655967834\n",
            "Epoch 7 completed, Loss: 0.18967113366469424\n",
            "Epoch 8 completed, Loss: 0.0901393237067981\n",
            "Epoch 9 completed, Loss: 0.060291363491102104\n",
            "Epoch 10 completed, Loss: 0.024450086859708772\n"
          ]
        }
      ],
      "source": [
        "# Question 1\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "model = CNN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training the model\n",
        "def train_model(num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1} completed, Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTiqtc-X6Y9i",
        "outputId": "61be81da-d19b-43c2-e79c-8596fcd693c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reclassification Performance: 99.9%\n"
          ]
        }
      ],
      "source": [
        "# Question 1\n",
        "\n",
        "def evaluate_model(data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Reclassification Performance: {accuracy}%')\n",
        "\n",
        "evaluate_model(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tYeq-LsKaAnk"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNWithDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithDropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout(0.25)  # Adding dropout\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 512)\n",
        "        self.dropout2 = nn.Dropout(0.5)  # Adding dropout\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZT1Oc-7aWFu",
        "outputId": "455932f8-8b7f-436c-a2ed-a1749c5404f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed, Loss: 1.3889086791809568\n",
            "Epoch 2 completed, Loss: 1.0200879307503397\n",
            "Epoch 3 completed, Loss: 0.8458650999880851\n",
            "Epoch 4 completed, Loss: 0.7090086718188956\n",
            "Epoch 5 completed, Loss: 0.5668134652870767\n",
            "Epoch 6 completed, Loss: 0.48024125872774326\n",
            "Epoch 7 completed, Loss: 0.3306287279788484\n",
            "Epoch 8 completed, Loss: 0.2682206523703768\n",
            "Epoch 9 completed, Loss: 0.22305361783884942\n",
            "Epoch 10 completed, Loss: 0.14974628698001516\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "# Question 2\n",
        "\n",
        "# Load your model\n",
        "model = CNNWithDropout()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training the model\n",
        "def train_model():\n",
        "    model.train()\n",
        "    for epoch in range(10):  # Continue with 10 epochs\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1} completed, Loss: {running_loss / len(train_loader)}')\n",
        "    print('Training complete')\n",
        "\n",
        "train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YXDmEaiacyF",
        "outputId": "f247c92c-112a-4a05-aa8a-5cc6869673d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reclassification Performance: 98.63333333333334%\n"
          ]
        }
      ],
      "source": [
        "# Question 2\n",
        "\n",
        "def evaluate_model(data_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "reclassification_performance = evaluate_model(train_loader)\n",
        "print(f'Reclassification Performance: {reclassification_performance}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ_2_TdSI0Gx"
      },
      "source": [
        "**Question 2**\n",
        "\n",
        "**Why Decreased Performance Standard Deviation Indicates Robustness:**\n",
        "A model with a lower standard deviation in performance across different runs or datasets is considered more robust because it shows consistent results regardless of slight variations in the input data. This consistency is crucial in practical applications where input data can vary in quality or specifics. Regularization techniques like dropout help achieve lower variance in performance by preventing the model from overfitting to noise or specific details in the training set, thereby improving the model's ability to generalize to new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_CoHt_p5G5Rh"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "\n",
        "class CNNWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithBatchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 512)\n",
        "        self.bn3 = nn.BatchNorm1d(512)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = F.relu(self.bn3(self.fc1(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net_with_bn = CNNWithBatchNorm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux22Z2qQJPBT",
        "outputId": "04c8014f-52af-45c9-e336-02494b0f45bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 1.9306779947686703, val_loss: 1.8270292635116057\n",
            "Epoch 2 loss: 1.929624136458052, val_loss: 1.8260516229686\n",
            "Epoch 3 loss: 1.9288318829333528, val_loss: 1.826689156149949\n",
            "Epoch 4 loss: 1.9162208577419848, val_loss: 1.8246269557361994\n",
            "Epoch 5 loss: 1.920689142764883, val_loss: 1.8264216578359755\n",
            "Early stopping at epoch 5\n"
          ]
        }
      ],
      "source": [
        "# Question 3\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(20):  # increase epochs since we're using early stopping\n",
        "    net_with_bn.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net_with_bn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation loss\n",
        "    net_with_bn.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            outputs = net_with_bn(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    print(f'Epoch {epoch + 1} loss: {running_loss / len(train_loader)}, val_loss: {val_loss / len(test_loader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gA39pywRaihU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251b0917-2b94-4193-c615-8be6daecf2bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reclassification Performance: 98.63333333333334%\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(data_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "reclassification_performance = evaluate_model(train_loader)\n",
        "print(f'Reclassification Performance: {reclassification_performance}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}