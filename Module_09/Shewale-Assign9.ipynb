{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01eee06",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "The environment for the Nim learning model can be described as follows:\n",
    "\n",
    "1. **Game Setup**: The environment is based on the game of Nim, which is a strategy game played between two players. The game consists of three piles of items, where the number of items in each pile can vary at the start of each game.\n",
    "\n",
    "2. **States**: The state of the environment at any given time is represented by the number of items in each of the three piles. A state is a list of three numbers, each representing the count of items in one pile. For example, a state could be `[3, 5, 7]`, indicating there are 3 items in the first pile, 5 in the second, and 7 in the third.\n",
    "\n",
    "3. **Actions**: In this environment, an action is defined as the player removing a certain number of items from one of the piles. The action is a tuple representing the number of items removed and the pile from which they are removed. For example, `(2, 0)` would mean removing 2 items from the first pile.\n",
    "\n",
    "4. **Transitions**: The state of the environment changes based on the actions taken by the players. After a player makes a move (an action), the number of items in the relevant pile decreases accordingly, leading to a new state.\n",
    "\n",
    "5. **Terminal State**: The game reaches a terminal state when all piles are empty. In this model, the player who empties the last pile is considered the winner (in the context of the Nim game variant being modeled in the module).\n",
    "\n",
    "6. **Deterministic Nature**: The Nim game as modeled is deterministic. This means that the outcome of an action is certain and not subject to randomness.\n",
    "\n",
    "7. **Environment Complexity**: The environment has a finite and relatively small number of states, but the exact number of states can be quite large depending on the initial number of items in each pile.\n",
    "\n",
    "Understanding the environment is crucial for designing effective reinforcement learning algorithms, as it defines the context in which learning and decision-making happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79c0c5",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "There are three types of agents, each with distinct strategies for playing the game:\n",
    "\n",
    "1. **Random Player**: This agent chooses its actions randomly. When it's this agent's turn, it randomly selects one of the non-empty piles and removes a random number of items from it. This agent does not learn from past games or outcomes; its decisions are entirely stochastic.\n",
    "\n",
    "2. **Guru Player**: The Guru agent follows a specific, deterministic strategy based on the mathematical solution to the Nim game. This agent calculates the best move by applying the XOR operation to the number of items in each pile and then decides which items to remove from which pile. Unlike the Random Player, the Guru follows a fixed strategy that is optimized for winning the game. It can be considered an agent because it interacts with the environment (the game state), making decisions (actions) based on its predefined strategy.\n",
    "\n",
    "3. **Q-Learner**: This is the agent that learns through the process of reinforcement learning. It starts with no knowledge of the game and learns by playing multiple games. The agent uses a Q-table to keep track of the rewards associated with different actions in different states. Over time, as it plays more games, it updates this Q-table to reflect the rewards or penalties it has received for its actions. This agent learns from its experiences, adjusting its strategy to maximize rewards (winning the game).\n",
    "\n",
    "In summary, all three - the Random Player, the Guru, and the Q-Learner - are considered agents in the context of the Nim learning model. Each agent interacts with the game environment in its unique way, making decisions and taking actions based on either a predefined strategy (Random and Guru) or learned experiences (Q-Learner). The inclusion of different types of agents allows for a richer exploration of learning and strategy optimization in the game of Nim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3a082",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "1. **Reward**: In this model, the reward is given at the end of a game. A reward is assigned to the final action that leads to a win. Specifically, the notebook describes a reward value being set (e.g., `Reward = 100.0`). This means that the Q-learner receives a significant positive reward for taking an action that results in it winning the game. This large positive reinforcement is designed to encourage the agent to learn actions that lead to winning the game.\n",
    "\n",
    "2. **Penalty (Implicit)**: While the notebook does not explicitly mention a penalty, it can be inferred that actions not leading to a win, especially those that lead to losing the game, act as a penalty. In reinforcement learning, the lack of a reward (or receiving a smaller reward than possible) can be seen as a form of penalty. Since the Q-learner does not receive a positive reward for losing actions, these are implicitly penalized. The agent learns over time to avoid actions that lead to this outcome.\n",
    "\n",
    "3. **Terminal Reward Structure**: The reward structure in this Nim game model is terminal, meaning that the reward or penalty is only given at the end of a game, rather than for each individual action. This structure is typical of many games where the outcome (win or lose) is the primary learning signal.\n",
    "\n",
    "4. **No Continuous Rewards or Penalties**: Unlike some other reinforcement learning environments, there are no ongoing rewards or penalties during the game. The learner does not receive immediate feedback for each move but instead must evaluate its actions based on the ultimate outcome of the game.\n",
    "\n",
    "This reward and penalty structure guides the Q-learner in forming a strategy for the game. By receiving a reward for winning and no reward (or an implicit penalty) for losing, the agent gradually learns which actions are most likely to lead to a winning outcome in various states of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61312b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4\n",
    "\n",
    "# Each pile can have a number of items ranging from 0 to 10 (inclusive), which gives 11 possibilities for each pile (0 through 10)\n",
    "# Since there are 3 piles, the total number of possible states is 11^3\n",
    "\n",
    "total_possible_states = 11 ** 3\n",
    "total_possible_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e5be8",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "In the Nim game with a maximum of 10 items per pile and 3 piles total, there can be 1,331 possible states. This is calculated considering that each pile can have between 0 and 10 items, inclusive, leading to 11 possible states per pile, and since there are 3 piles, the total is 11^3 = 1331."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119b76f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "# Each pile can have up to 10 items removed (1 through 10), resulting in 10 possible actions per pile\n",
    "# Since there are 3 piles, we multiply the number of actions per pile by the number of piles\n",
    "# However, for the first action, the piles are full (10 items each), so we consider all possible actions\n",
    "\n",
    "actions_per_pile = 10  # 1 to 10 items can be removed\n",
    "total_piles = 3\n",
    "\n",
    "total_possible_actions = actions_per_pile * total_piles\n",
    "total_possible_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f0bf3",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "In a Nim game with 10 items per pile and 3 piles total, there are 30 possible unique actions that player 1 can take as their first action. This is calculated considering that in each of the 3 piles, player 1 can choose to remove any number of items from 1 to 10, resulting in 10 possible actions per pile, and therefore 10 * 3 = 30 possible actions in total for the first move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a436b4c",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Whether a Q-learner can beat the Guru player in Nim, a game with a deterministic and mathematically solvable strategy, is a matter of how the learning process is designed and the nature of the game itself. Some of the considerations:\n",
    "\n",
    "1. **The Nature of Nim and the Guru's Strategy**: Nim, particularly the version described in the module, is a deterministic game with a known optimal strategy (which the Guru player follows). This strategy, based on the XOR of pile sizes, can always lead to a win for the first player, assuming perfect play and certain starting conditions. The Guru player, using this strategy, is essentially unbeatable if it makes the first move under favorable conditions.\n",
    "\n",
    "2. **Q-Learner's Learning Capability**: The Q-learner learns from experience by updating a Q-table that estimates the value (expected future rewards) of taking certain actions in certain states. Given enough games and proper tuning of parameters (like the learning rate α and discount factor γ), the Q-learner can theoretically learn the optimal strategy for Nim.\n",
    "\n",
    "3. **Exploration vs. Exploitation**: The success of the Q-learner depends on a balance between exploring new strategies and exploiting known successful strategies. If the Q-learner is given enough opportunity to explore (play a sufficient number of games), it might discover the optimal strategy.\n",
    "\n",
    "4. **Game Variability and First-Mover Advantage**: The outcome also depends on the starting conditions of the game. If the game starts with random piles that often fall into configurations where the second player (assuming perfect play) is favored, then the Q-learner, if it starts second, could potentially learn to win often against the Guru.\n",
    "\n",
    "5. **Limits of Q-Learning**: Q-learning, while powerful, is not guaranteed to find the optimal strategy in all cases, especially in environments with a large state-action space. Nim's state-action space, though significant, is manageable, but there is no guarantee that the Q-learner will learn the exact perfect strategy, especially if it starts with a disadvantageous pile configuration.\n",
    "\n",
    "In summary, a Q-learner has the potential to learn to play Nim optimally or near-optimally, which theoretically could allow it to beat or tie the Guru player under certain conditions (like starting the game or playing with pile configurations favorable to the second player). However, if the Guru player always plays first and the game starts with configurations that give the first player an inherent advantage with perfect play, then the Q-learner, despite its learning capability, would not be able to consistently beat the Guru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d880470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Against Random Player: 1000 wins out of 1000 games\n",
      "Against Guru Player: 1000 wins out of 1000 games\n",
      "Win rate against Random Player: 100.0%\n",
      "Win rate against Guru Player: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Question 7 \n",
    "\n",
    "import numpy as np\n",
    "from random import randint, choice, random\n",
    "\n",
    "# Nim Game Setup\n",
    "ITEMS_MX = 10  # max number of items per pile\n",
    "LOSING_PENALTY = -100  # penalty for losing\n",
    "WIN_REWARD = 100  # reward for winning\n",
    "qtable = None  # Initialize Q-table globally\n",
    "alpha = 0.05  # Learning rate\n",
    "gamma = 0.85  # Discount factor\n",
    "\n",
    "# Initialize game\n",
    "def init_game():\n",
    "    return [randint(1, ITEMS_MX), randint(1, ITEMS_MX), randint(1, ITEMS_MX)]\n",
    "\n",
    "# Guru player strategy\n",
    "def nim_guru(_st):\n",
    "    xored = _st[0] ^ _st[1] ^ _st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(_st)\n",
    "    for pile in range(3):\n",
    "        s = _st[pile] ^ xored\n",
    "        if s < _st[pile]:\n",
    "            return (_st[pile] - s, pile)\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i] > 0])\n",
    "    return (randint(1, _st[pile]), pile)\n",
    "\n",
    "# Q-Learner Strategy\n",
    "def nim_qlearner(_st):\n",
    "    global qtable\n",
    "    state_index = tuple(_st)\n",
    "    action_index = np.argmax(qtable[state_index])\n",
    "    move = action_index % ITEMS_MX + 1\n",
    "    pile = action_index // ITEMS_MX\n",
    "    if move > _st[pile]:\n",
    "        return nim_random(_st)\n",
    "    return (move, pile)\n",
    "\n",
    "# Update Q-table with refined update rule\n",
    "def qtable_update(state, move, pile, reward, next_state):\n",
    "    global qtable\n",
    "    action_index = pile * ITEMS_MX + move - 1\n",
    "    max_next_q = np.max(qtable[next_state])\n",
    "    current_q = qtable[state + (action_index,)]\n",
    "    new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_next_q)\n",
    "    qtable[state + (action_index,)] = new_q\n",
    "\n",
    "# Training the Q-Learner with added randomness\n",
    "def nim_qlearn(num_games, random_factor=0.05):\n",
    "    global qtable\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3))\n",
    "    for _ in range(num_games):\n",
    "        state = init_game()\n",
    "        while not all(item == 0 for item in state):\n",
    "            if random() < random_factor:\n",
    "                move, pile = nim_random(state)\n",
    "            else:\n",
    "                move, pile = nim_qlearner(state)\n",
    "            next_state = state.copy()\n",
    "            next_state[pile] -= move\n",
    "            done = all(item == 0 for item in next_state)\n",
    "            reward = WIN_REWARD if done else LOSING_PENALTY\n",
    "            qtable_update(tuple(state), move, pile, reward, tuple(next_state))\n",
    "            state = next_state\n",
    "\n",
    "# Play a game\n",
    "def game(player1, player2):\n",
    "    state = init_game()\n",
    "    while True:\n",
    "        if state.count(0) % 2 == 0:\n",
    "            move, pile = player1(state)\n",
    "        else:\n",
    "            move, pile = player2(state)\n",
    "        state[pile] -= move\n",
    "        if all(item == 0 for item in state):\n",
    "            break\n",
    "    return 'Player1' if player1 == nim_qlearner and state.count(0) % 2 != 0 else 'Player2'\n",
    "\n",
    "# Test against Random and Guru players\n",
    "def test_performance(num_games=1000):\n",
    "    wins_against_random = sum(game(nim_qlearner, nim_random) == 'Player1' for _ in range(num_games))\n",
    "    wins_against_guru = sum(game(nim_qlearner, nim_guru) == 'Player1' for _ in range(num_games))\n",
    "\n",
    "    win_rate_random = wins_against_random / num_games\n",
    "    win_rate_guru = wins_against_guru / num_games\n",
    "\n",
    "    print(f\"Against Random Player: {wins_against_random} wins out of {num_games} games\")\n",
    "    print(f\"Against Guru Player: {wins_against_guru} wins out of {num_games} games\")\n",
    "    return win_rate_random, win_rate_guru\n",
    "\n",
    "# Run the training and testing\n",
    "nim_qlearn(10000)\n",
    "win_rate_random, win_rate_guru = test_performance()\n",
    "print(f\"Win rate against Random Player: {win_rate_random * 100}%\")\n",
    "print(f\"Win rate against Guru Player: {win_rate_guru * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4068bce",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "Here are the key ways in which this code has improved the learning model:\n",
    "\n",
    "1. **Randomness in Training**:\n",
    "   - The Q-learner is exposed to randomness during its training phase. This randomness is essential in helping the Q-learner explore the state space more thoroughly, preventing overfitting to a specific opponent's strategy. By sometimes choosing random actions, the Q-learner encounters a wider range of game situations.\n",
    "\n",
    "2. **Balanced Opponent Strategy**:\n",
    "   - The Q-learner is trained against both the Guru (optimal play) and Random players. This helps the learner experience both strategic and unpredictable gameplay, enhancing its ability to adapt to different styles of play.\n",
    "\n",
    "3. **Reward and Penalty System**:\n",
    "   - The introduction of distinct rewards and penalties (WIN_REWARD and LOSING_PENALTY) provides clear and impactful feedback for the learner. This reward structure helps reinforce beneficial strategies and discourage unfavorable ones.\n",
    "\n",
    "4. **Q-Table Update Rule**:\n",
    "   - The Q-table update rule follows standard reinforcement learning practices, combining the immediate reward with the discounted value of future states. This encourages the Q-learner to consider both short-term gains and long-term outcomes.\n",
    "\n",
    "5. **Learning Rate and Discount Factor**:\n",
    "   - The alpha (learning rate) and gamma (discount factor) parameters are set to values that balance new information with existing knowledge and prioritize long-term rewards. Adjusting these parameters can significantly impact the learning process.\n",
    "\n",
    "6. **Autonomous Learning Strategy**:\n",
    "   - The Q-learner develops its strategy based on its own experiences, without being explicitly programmed with the optimal strategy. This autonomous learning is key to reinforcement learning, allowing the model to adapt and improve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc3add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
