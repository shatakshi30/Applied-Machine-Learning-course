{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87de2d84",
   "metadata": {},
   "source": [
    "**Assignment 2**\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "1. **Perceptron**:\n",
    "   - **Speed**: Generally fast, especially in its basic form.\n",
    "   - **Strength**: Good for simple, linearly separable problems.\n",
    "   - **Robustness**: Not very robust to non-linear data; sensitive to noisy data and outliers.\n",
    "   - **Feature Type**: Works best with numerical features as it relies on linear combinations of features.\n",
    "   - **Statistical Nature**: It's more geometric in nature, as it tries to find a separating hyperplane.\n",
    "   - **Optimization Problem**: Yes, it minimizes misclassifications. The cost function is usually based on the distance of misclassified points from the decision boundary.\n",
    "\n",
    "2. **Support Vector Machine (SVM)**:\n",
    "   - **Speed**: Can be slow, especially for large datasets, due to the computation of support vectors.\n",
    "   - **Strength**: Very effective for high-dimensional spaces and when there is a clear margin of separation.\n",
    "   - **Robustness**: More robust than Perceptron, especially with kernel SVM that can handle non-linear data.\n",
    "   - **Feature Type**: Like Perceptron, favors numerical features. Can handle non-linear relationships with kernel trick.\n",
    "   - **Statistical Nature**: It's a margin-based classifier and can be viewed both geometrically and statistically.\n",
    "   - **Optimization Problem**: Yes, it maximizes the margin between classes. The cost function involves maximizing this margin while penalizing misclassifications.\n",
    "\n",
    "3. **Decision Tree**:\n",
    "   - **Speed**: Fast for training, but prediction speed can vary depending on the tree depth.\n",
    "   - **Strength**: Good for data with complex relationships and interactions between features.\n",
    "   - **Robustness**: Can overfit, especially with noisy data and without proper pruning.\n",
    "   - **Feature Type**: Can handle both numerical and categorical data effectively.\n",
    "   - **Statistical Nature**: More heuristic-based, using algorithms like ID3, C4.5, etc., to build the tree.\n",
    "   - **Optimization Problem**: Not in the traditional sense. It uses heuristics like information gain or Gini impurity to split nodes.\n",
    "\n",
    "4. **Random Forest**:\n",
    "   - **Speed**: Slower for training due to building multiple trees, but parallelizable. Fast for predictions.\n",
    "   - **Strength**: Very powerful, as it combines multiple decision trees to improve performance.\n",
    "   - **Robustness**: More robust than a single decision tree, as it reduces overfitting through ensemble learning.\n",
    "   - **Feature Type**: Handles both numerical and categorical data well.\n",
    "   - **Statistical Nature**: Statistical, as it is an ensemble method that relies on the law of large numbers.\n",
    "   - **Optimization Problem**: Like decision trees, it's more heuristic-based, using the combined decisions of multiple trees.\n",
    "\n",
    "**Which to try first on your dataset?**\n",
    "- This depends on the nature of your dataset. If our data is high-dimensional and linearly separable, SVM might be a good start. For more complex relationships or a mix of feature types, a Random Forest could be more suitable. If er are dealing with a large dataset and require fast training, starting with a simpler model like Perceptron or Decision Tree might be advantageous. Usually, starting with simpler models and then moving to more complex ones like Random Forest is a pragmatic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1842fc8",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "1. **Numerical**:\n",
    "   - **Definition**: Features that are measured on a numeric scale. They can be either discrete (countable items) or continuous (measurable quantities).\n",
    "   - **Example**: From the Iris dataset, the feature 'Petal Length' is a numerical feature. It is measured in centimeters and is a continuous variable. Example values might be 1.4, 4.5, 5.1 cm, etc.\n",
    "\n",
    "2. **Nominal**:\n",
    "   - **Definition**: Features that represent categories without any intrinsic ordering. They are also known as categorical variables.\n",
    "   - **Example**: In a dataset about cars, the feature 'Car Brand' could be nominal. Example values might be 'Toyota', 'Ford', 'Honda', etc.\n",
    "\n",
    "3. **Date**:\n",
    "   - **Definition**: Features that represent dates or times.\n",
    "   - **Example**: In a sales dataset, the feature 'Transaction Date' would be a date feature. Example values might be '2023-01-15', '2023-02-20', etc.\n",
    "\n",
    "4. **Text**:\n",
    "   - **Definition**: Features that contain textual data. This type of data is unstructured and often requires special processing for analysis.\n",
    "   - **Example**: In a dataset of customer reviews, the feature 'Review Text' is a text feature. Example values might be \"Great product, highly recommend!\" or \"Poor quality, arrived late.\"\n",
    "\n",
    "5. **Image**:\n",
    "   - **Definition**: Features that are composed of image data, typically stored in formats like JPEG, PNG, etc.\n",
    "   - **Example**: In a dataset for a facial recognition system, the feature might be 'Face Image'. The values would be the actual images of individuals' faces.\n",
    "\n",
    "6. **Dependent Variable**:\n",
    "   - **Definition**: The variable that you are trying to predict or explain in your dataset. It's the outcome or the variable whose variation you're interested in.\n",
    "   - **Example**: In the Iris dataset, the dependent variable could be 'Species' which is the type of iris plant (Setosa, Versicolour, Virginica). This is what you're trying to predict based on other features like petal length, petal width, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f926670",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Several other metrics are commonly used to evaluate classifier performance. Each of these metrics offers different insights into the strengths and weaknesses of a classification model.\n",
    "\n",
    "1. **Precision**: This metric indicates the proportion of positive identifications that were actually correct. It is calculated as the number of true positives (TP) divided by the total number of positive predictions (TP + false positives (FP)). Precision is particularly important in scenarios where the cost of false positives is high.\n",
    "\n",
    "2. **Recall (Sensitivity or True Positive Rate)**: Recall measures how many of the actual positive cases were correctly identified by the classifier. It is calculated as TP divided by the total number of actual positives (TP + false negatives (FN)). This metric is crucial in situations where missing a positive instance is costly, such as medical diagnoses.\n",
    "\n",
    "3. **F1 Score**: The F1 score combines precision and recall into a single metric by taking their harmonic mean. It provides a balance between precision and recall, making it useful in scenarios where both false positives and false negatives are important.\n",
    "\n",
    "4. **AUC-ROC (Area Under the Receiver Operating Characteristics Curve)**: This metric is used to evaluate the performance of a binary classifier. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The AUC represents a classifier's ability to distinguish between classes, with higher values indicating better performance.\n",
    "\n",
    "5. **Log Loss (Logistic Loss or Cross-Entropy Loss)**: This is used to assess the performance of classification problems, particularly in probabilistic classifiers. It penalizes false classifications by considering the uncertainty of the prediction based on the probability estimate.\n",
    "\n",
    "Understanding these metrics helps in evaluating classifier performance more comprehensively, especially in scenarios where accuracy alone might be misleading, such as with imbalanced datasets. The choice of metric often depends on the specific needs and context of the classification task at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126eebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.83306</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.80261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.69559</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.69559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.71125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.83306</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.80261</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.71125</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GRE Score TOEFL Score University Rating       SOP      LOR   \\\n",
       "GRE Score               1.0    0.835977          0.668976  0.612831  0.557555   \n",
       "TOEFL Score        0.835977         1.0           0.69559  0.657981  0.567721   \n",
       "University Rating  0.668976     0.69559               1.0  0.734523  0.660123   \n",
       "SOP                0.612831    0.657981          0.734523       1.0  0.729593   \n",
       "LOR                0.557555    0.567721          0.660123  0.729593       1.0   \n",
       "CGPA                0.83306    0.828417          0.746479  0.718144  0.670211   \n",
       "Research           0.580391    0.489858          0.447783  0.444029  0.396859   \n",
       "Chance of Admit     0.80261    0.791594           0.71125  0.675732  0.669889   \n",
       "\n",
       "                       CGPA  Research Chance of Admit   \n",
       "GRE Score           0.83306  0.580391          0.80261  \n",
       "TOEFL Score        0.828417  0.489858         0.791594  \n",
       "University Rating  0.746479  0.447783          0.71125  \n",
       "SOP                0.718144  0.444029         0.675732  \n",
       "LOR                0.670211  0.396859         0.669889  \n",
       "CGPA                    1.0  0.521654         0.873289  \n",
       "Research           0.521654       1.0         0.553202  \n",
       "Chance of Admit    0.873289  0.553202              1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4 \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate the mean of a list\n",
    "def mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "# Function to calculate covariance between two lists\n",
    "def covariance(x, y):\n",
    "    mean_x, mean_y = mean(x), mean(y)\n",
    "    covar = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(len(x))) / (len(x) - 1)\n",
    "    return covar\n",
    "\n",
    "# Function to calculate the standard deviation of a list\n",
    "def standard_deviation(values):\n",
    "    mean_value = mean(values)\n",
    "    return (sum((x - mean_value) ** 2 for x in values) / (len(values) - 1)) ** 0.5\n",
    "\n",
    "# Function to calculate correlation between two lists\n",
    "def correlation(x, y):\n",
    "    stddev_x = standard_deviation(x)\n",
    "    stddev_y = standard_deviation(y)\n",
    "    return covariance(x, y) / (stddev_x * stddev_y)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('datasets/Admission_Predict.csv')\n",
    "\n",
    "# Exclude 'Serial No.' if present\n",
    "if 'Serial No.' in df.columns:\n",
    "    df = df.drop(columns=['Serial No.'])\n",
    "\n",
    "# Initialize an empty DataFrame for correlation matrix\n",
    "correlation_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "\n",
    "# Calculate correlations\n",
    "for column1 in df.columns:\n",
    "    for column2 in df.columns:\n",
    "        x = df[column1].tolist()\n",
    "        y = df[column2].tolist()\n",
    "        correlation_matrix.at[column1, column2] = correlation(x, y)\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abfc3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.802610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.711250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.711250</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
       "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
       "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
       "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
       "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
       "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
       "Research            0.580391     0.489858           0.447783  0.444029   \n",
       "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
       "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
       "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
       "SOP                0.729593  0.718144  0.444029          0.675732  \n",
       "LOR                1.000000  0.670211  0.396859          0.669889  \n",
       "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
       "Research           0.396859  0.521654  1.000000          0.553202  \n",
       "Chance of Admit    0.669889  0.873289  0.553202          1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's verify the correctness of the from-scratch correlation matrix using DataFrame.corr() method\n",
    "verification_corr_matrix = df.corr()\n",
    "\n",
    "verification_corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e90de",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "1. **Should we use 'Serial No.'?**\n",
    "   - No, 'Serial No.' should not be used. It is typically an arbitrary identifier assigned to each record and does not have any predictive or correlational relationship with the outcome variable. Including it could distort statistical analyses and predictive modeling because it does not contain meaningful information that contributes to the 'Chance of Admit'.\n",
    "\n",
    "2. **Why does the diagonal of the matrix have all 1's?**\n",
    "   - The diagonal of a correlation matrix represents the correlation of each variable with itself. Since any variable will always have a perfect linear relationship with itself, the correlation coefficient is 1.\n",
    "\n",
    "3. **Correlations between all the variables:**\n",
    "   - The correlations between the variables and the target variable 'Chance of Admit' indicate how each feature is related to the chances of admission. Features with higher correlation coefficients (closer to 1 or -1) have a stronger linear relationship with the target variable. Positive values indicate that as the feature increases, the chance of admit tends to increase, while negative values would indicate the opposite.\n",
    "\n",
    "4. **Most important variable to predict 'Chance of Admit':**\n",
    "   - Based on the provided correlation matrix, the 'CGPA' has the highest correlation coefficient (0.873289) with the 'Chance of Admit'. This suggests that 'CGPA' is the most important variable when it comes to predicting the chance of admission and would likely be a significant predictor in a model designed to predict admission chances. Variables like 'GRE Score' and 'TOEFL Score' also have relatively high correlations, which suggests they are also important, but 'CGPA' stands out as the most correlated feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc87e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
