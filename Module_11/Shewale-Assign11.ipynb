{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a7fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support itemsets\n",
      "0  0.228420  (alice)\n",
      "1  0.022314   (back)\n",
      "2  0.033470  (began)\n",
      "3  0.022314   (came)\n",
      "4  0.021139    (cat)\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Load the novel and preprocess\n",
    "sentences = gutenberg.sents('carroll-alice.txt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_sentences = [\n",
    "    [word.lower() for word in sentence if word.lower() not in stop_words and re.search(r'^[a-zA-Z]+$', word)]\n",
    "    for sentence in sentences\n",
    "    if sentence\n",
    "]\n",
    "\n",
    "# Convert to transaction data\n",
    "encoder = TransactionEncoder()\n",
    "transformed_data = encoder.fit_transform(processed_sentences)\n",
    "df = pd.DataFrame(transformed_data, columns=encoder.columns_)\n",
    "\n",
    "# Find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.02, use_colnames=True)\n",
    "\n",
    "# Export to CSV\n",
    "frequent_itemsets.to_csv('frequent_itemsets.csv', index=False)\n",
    "\n",
    "# Print some of the frequent itemsets\n",
    "print(frequent_itemsets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc6ac95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.228420</td>\n",
       "      <td>frozenset({'alice'})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022314</td>\n",
       "      <td>frozenset({'back'})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033470</td>\n",
       "      <td>frozenset({'began'})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022314</td>\n",
       "      <td>frozenset({'came'})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021139</td>\n",
       "      <td>frozenset({'cat'})</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support              itemsets\n",
       "0  0.228420  frozenset({'alice'})\n",
       "1  0.022314   frozenset({'back'})\n",
       "2  0.033470  frozenset({'began'})\n",
       "3  0.022314   frozenset({'came'})\n",
       "4  0.021139    frozenset({'cat'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the new CSV file containing frequent itemsets\n",
    "new_file_path = '/Users/shatakshishewale/Desktop/JHU/Spring 24/Applied ML/jupyter codes/frequent_itemsets.csv'\n",
    "new_frequent_itemsets = pd.read_csv(new_file_path)\n",
    "\n",
    "# Display the content of the dataset to understand its structure and contents\n",
    "new_frequent_itemsets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5609348e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.095713</td>\n",
       "      <td>(alice, said)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.034058</td>\n",
       "      <td>(thought, alice)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.032883</td>\n",
       "      <td>(mock, turtle)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.025250</td>\n",
       "      <td>(king, said)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.021726</td>\n",
       "      <td>(alice, little)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support          itemsets  length\n",
       "49  0.095713     (alice, said)       2\n",
       "50  0.034058  (thought, alice)       2\n",
       "52  0.032883    (mock, turtle)       2\n",
       "51  0.025250      (king, said)       2\n",
       "48  0.021726   (alice, little)       2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "# Convert string representation of frozenset to actual sets and filter itemsets with more than one element\n",
    "new_frequent_itemsets['itemsets'] = new_frequent_itemsets['itemsets'].apply(lambda x: eval(x))\n",
    "new_frequent_itemsets['length'] = new_frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "multi_word_itemsets = new_frequent_itemsets[new_frequent_itemsets['length'] > 1]\n",
    "\n",
    "# Display itemsets with more than one element\n",
    "multi_word_itemsets.sort_values(by='support', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4735190",
   "metadata": {},
   "source": [
    "**Report Analysis**\n",
    "\n",
    "Here are some of the most interesting patterns identified from the frequent itemsets of \"Alice in Wonderland\" by Lewis Carroll:\n",
    "\n",
    "1. **(said, alice)**: This phrase appears with a significant support value of approximately 9.57%, indicating that conversations involving Alice are frequently mentioned throughout the novel.\n",
    "2. **(thought, alice)**: Occurring with a support of about 3.41%, this phrase suggests recurring instances where Alice's thoughts or inner reflections are narrated.\n",
    "3. **(turtle, mock)**: With a support value of around 3.29%, this likely refers to the character \"Mock Turtle,\" a prominent figure in the story.\n",
    "4. **(king, said)**: This phrase, showing up with a support of about 2.53%, reflects frequent dialogues or statements involving the King character.\n",
    "5. **(little, alice)**: Occurring with a support of approximately 2.17%, this phrase might describe Alice, emphasizing her size or her perceived insignificance in various scenarios within the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58bcdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "        from numpy import fromfile, uint8\n",
    "        import os\n",
    "        import struct\n",
    "\n",
    "        labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "        images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "        with open(labels_path, 'rb') as lbpath:\n",
    "            magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "            labels = fromfile(lbpath, dtype=uint8)\n",
    "            with open(images_path, 'rb') as imgpath:\n",
    "                magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "                images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "                images = ((images / 255.) - .5) * 2\n",
    "        return images, labels\n",
    "    \n",
    "\n",
    "X_train_mnist, y_train_mnist = load_mnist('/Users/shatakshishewale/Desktop/JHU/Spring 24/Applied ML/jupyter codes/datasets/', kind='train')\n",
    "print(f'Rows= {X_train_mnist.shape[0]}, columns= {X_train_mnist.shape[1]}')\n",
    "\n",
    "X_test_mnist, y_test_mnist = load_mnist('/Users/shatakshishewale/Desktop/JHU/Spring 24/Applied ML/jupyter codes/datasets/', kind='t10k')\n",
    "print(f'Rows= {X_test_mnist.shape[0]}, columns= {X_test_mnist.shape[1]}')\n",
    "\n",
    "# Split the training data for training and validation\n",
    "X_train, X_valid = X_train_mnist[:55000], X_train_mnist[55000:]\n",
    "y_train, y_valid = y_train_mnist[:55000], y_train_mnist[55000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610f816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50/50 | Cost: 6671.35 | Train/Valid Acc.: 98.49%/97.38%  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.99%\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    def __init__(self, n_hidden1=30, n_hidden2=30, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.w_out, self.w_h2, self.w_h1 = None, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def onehot(y, n_classes):\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def _forward(self, X):\n",
    "        z_h1 = np.dot(X, self.w_h1)\n",
    "        a_h1 = self.sigmoid(z_h1)\n",
    "        z_h2 = np.dot(a_h1, self.w_h2)\n",
    "        a_h2 = self.sigmoid(z_h2)\n",
    "        z_out = np.dot(a_h2, self.w_out)\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        return z_h1, a_h1, z_h2, a_h2, z_out, a_out\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1 - y_enc) * (np.log(1 - output))\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, _, _, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(a_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden2, n_output))\n",
    "        self.w_h2 = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden1, self.n_hidden2))\n",
    "        self.w_h1 = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden1))\n",
    "        \n",
    "        y_train_enc = self.onehot(y_train, n_output)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                _, a_h1, _, a_h2, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "\n",
    "                # Backpropagation\n",
    "                sigma_out = a_out - y_train_enc[batch_idx]\n",
    "                sigmoid_derivative_h2 = a_h2 * (1 - a_h2)\n",
    "                sigma_h2 = np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h2\n",
    "                \n",
    "                sigmoid_derivative_h1 = a_h1 * (1 - a_h1)\n",
    "                sigma_h1 = np.dot(sigma_h2, self.w_h2.T) * sigmoid_derivative_h1\n",
    "\n",
    "                grad_w_out = np.dot(a_h2.T, sigma_out)\n",
    "                grad_w_h2 = np.dot(a_h1.T, sigma_h2)\n",
    "                grad_w_h1 = np.dot(X_train[batch_idx].T, sigma_h1)\n",
    "\n",
    "                self.w_out -= self.eta * grad_w_out\n",
    "                self.w_h2 -= self.eta * grad_w_h2\n",
    "                self.w_h1 -= self.eta * grad_w_h1\n",
    "\n",
    "            # Evaluation after each epoch\n",
    "            _, _, _, _, _, a_out = self._forward(X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(float) / X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(float) / X_valid.shape[0])\n",
    "            sys.stderr.write(f'\\r{epoch+1}/{self.epochs} | Cost: {cost:.2f} | Train/Valid Acc.: {train_acc*100:.2f}%/{valid_acc*100:.2f}% ')\n",
    "            sys.stderr.flush()\n",
    "\n",
    "        return self\n",
    "\n",
    "# Instantiate the model\n",
    "nn = NeuralNetMLP(n_hidden1=50, n_hidden2=50, epochs=50, eta=0.001, minibatch_size=64, seed=1)\n",
    "\n",
    "# Fit the model to the data\n",
    "nn.fit(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = nn.predict(X_test_mnist)\n",
    "accuracy = np.sum(y_test_mnist == y_pred).astype(float) / X_test_mnist.shape[0]\n",
    "print(f'Accuracy on test set: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b9320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
